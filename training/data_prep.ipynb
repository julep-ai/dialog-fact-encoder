{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8198fee9-000e-4ef9-bb13-82c649c2e816",
   "metadata": {},
   "source": [
    "## Data prep for retrieving beliefs for dialogs\n",
    "\n",
    "**Goal:** Train an embedding model to match dialogs with beliefs and facts\n",
    "**Method:**\n",
    "- [x] Use stacked_samsum + dialogsum as datasets\n",
    "- [x] Prepare datasets\n",
    "    - [x] remove unnecessary columns\n",
    "    - [x] remove '#' from dialogsum\n",
    "    - [x] expand the stacked dataset\n",
    "    - [x] truncate on the left to 512 tokens\n",
    "    - [x] combine and train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ec397-3b13-4e2b-8e0f-9cf127378b8f",
   "metadata": {},
   "source": [
    "### Imports and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94dea7bd-f87b-4559-bd82-dadf3dfd6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "query_prefix = \"Represent this sentence for searching relevant passages:\"\n",
    "max_len = 512\n",
    "next_concept_sep = \"\\n[NEXT_CONCEPT]\\n\"\n",
    "training_input_data = \"./data/output.jsonl\"\n",
    "eval_input_data = \"./data/eval-output.jsonl\"\n",
    "training_hn_data = \"./data/hn-output.jsonl\"\n",
    "eval_size = 12_500\n",
    "combined_data_path = \"./data/combined\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53fc09-0942-4e9a-921c-3804a1ede8ac",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e40d321-04e7-4048-891c-6748c6354a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from functools import partial\n",
    "import os\n",
    "import random\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "from FlagEmbedding import FlagModel\n",
    "from FlagEmbedding.baai_general_embedding.finetune.hn_mine import find_knn_neg\n",
    "import jsonlines as jsonl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb6f55d-ec09-4bc5-8f1a-31e521ad3121",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "#### Initial run\n",
    "\n",
    "- stacked_samsum: to be used as training\n",
    "- dialogsum: to be used as testing\n",
    "\n",
    "#### Final run\n",
    "\n",
    "- combined: stacked_samsum + dialogsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5420aa-d327-4d3a-8e02-90473dcca1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"stacked_samsum\": load_dataset(\n",
    "        \"stacked-summaries/stacked-samsum-1024\", \n",
    "        split=\"train+validation+test\",\n",
    "    ).remove_columns(['chapter_length', 'summary_length', 'is_stacked',]).filter(\n",
    "        lambda row: row[\"dialogue\"]\n",
    "    ).map(\n",
    "        lambda row: dict(dialogue=row[\"dialogue\"].replace(\"\\r\\n\", '\\n'))\n",
    "    ),\n",
    "    \"dialogsum\": load_dataset(\n",
    "        \"knkarthick/dialogsum\", \n",
    "        split=\"train+validation+test\",\n",
    "    ).remove_columns([\"id\", \"topic\"]).map(\n",
    "        lambda row: dict(dialogue=row[\"dialogue\"].replace(\"#\", ''))\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999a7a8f-6875-49ea-bda2-c54602f0748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(row):\n",
    "\n",
    "    dialogue = row[\"dialogue\"]\n",
    "    tokens = tokenizer.encode(dialogue, add_special_tokens=False)\n",
    "\n",
    "    return dict(token_count=len(tokens))\n",
    "\n",
    "datasets[\"stacked_samsum\"] = datasets[\"stacked_samsum\"].map(count_tokens)\n",
    "datasets[\"dialogsum\"] = datasets[\"dialogsum\"].map(count_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ab49b0-a246-428b-8d7d-524341bcb4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_dir(dialogue, left=False):\n",
    "    lines = dialogue.split(\"\\n\")\n",
    "    \n",
    "    toks_by_line = [\n",
    "        len(tokenizer.encode(line, add_special_tokens=False))\n",
    "        for line in lines\n",
    "    ]\n",
    "\n",
    "    idx = 0 if left else -1\n",
    "    \n",
    "    while sum(toks_by_line) > max_len:\n",
    "        toks_by_line.pop(idx)\n",
    "        lines.pop(idx)\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def expand_stacked(rows):\n",
    "    dialogues = rows[\"dialogue\"]\n",
    "    summaries = rows[\"summary\"]\n",
    "\n",
    "    final_dialogues = []\n",
    "    final_summaries = []\n",
    "    \n",
    "    for dialogue, summary in zip(dialogues, summaries):\n",
    "        \n",
    "        ss = summary.split(next_concept_sep)\n",
    "        dd = [\n",
    "            truncate_dir(dialogue, left=(i >= (len(ss) // 2)))\n",
    "            for i in range(len(ss))\n",
    "        ]\n",
    "\n",
    "        final_dialogues += dd\n",
    "        final_summaries += ss\n",
    "\n",
    "    return dict(\n",
    "        dialogue=final_dialogues,\n",
    "        summary=final_summaries,\n",
    "        token_count=[None]*len(final_summaries),\n",
    "    )\n",
    "\n",
    "datasets[\"stacked_samsum\"] = datasets[\"stacked_samsum\"].map(expand_stacked, batched=True).remove_columns([\"token_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1080dbe1-8958-4e13-93f7-ecfbf8da814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = concatenate_datasets(list(datasets.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a557a00b-c593-4f54-b84e-8b7f9406d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22beb7aa-f191-4660-a860-ef4169c229b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e4e674b6a14e599078cb2d713484ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/125705 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb99013354a4c3b9dc4c280a7047c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/13968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined.save_to_disk(combined_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f167345-c049-4521-a199-64c276979300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined = load_from_disk(combined_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767a4251-fab6-47ce-8cdc-e2416d70b440",
   "metadata": {},
   "source": [
    "### Prepare dataset for finetuning\n",
    "[Docs](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune)\n",
    "\n",
    "Format:\n",
    "```json\n",
    "{\"query\": str, \"pos\": List[str], \"neg\":List[str]}\n",
    "```\n",
    "\n",
    "Keys:\n",
    "- query: belief\n",
    "- pos: list of matching conversations\n",
    "- neg: list of random conversations from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10817e24-a6b5-49da-b1e7-6101b32a9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random(split=\"train\", far_from=0):\n",
    "    ds = combined[split]\n",
    "    ds_len = len(ds)\n",
    "    mid = ds_len // 2\n",
    "    which_half = far_from // mid\n",
    "    \n",
    "    start = (1 - which_half) * mid\n",
    "    end = ds_len - which_half * mid\n",
    "    idx = random.randrange(start, end)\n",
    "    \n",
    "    return ds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bf3bf97-86c4-41f4-ab07-7de94ed72344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20b849464484472b192d04719a6666a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with jsonl.open(training_input_data, mode='w') as writer:\n",
    "    for i, row in enumerate(tqdm(combined[\"train\"], total=len(combined[\"train\"]))):\n",
    "        query = row[\"summary\"]\n",
    "        pos = [row[\"dialogue\"]]\n",
    "    \n",
    "        neg = [\n",
    "            pick_random(split=\"train\", far_from=i)[\"dialogue\"]\n",
    "            for _ in range(3)\n",
    "        ]\n",
    "        \n",
    "        writer.write(dict(query=query, pos=pos, neg=neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e07bc44f-302c-4c7c-b7c6-62c9cd9db3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80695383928d4745ac0b51a1e726f0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with jsonl.open(eval_input_data, mode='w') as writer:\n",
    "    for i, row in enumerate(tqdm(combined[\"test\"], total=len(combined[\"train\"]))):\n",
    "        if i > eval_size:\n",
    "            break\n",
    "\n",
    "        query = row[\"summary\"]\n",
    "        pos = [row[\"dialogue\"]]\n",
    "    \n",
    "        neg = [\n",
    "            pick_random(split=\"test\", far_from=i)[\"dialogue\"]\n",
    "            for _ in range(3)\n",
    "        ]\n",
    "        \n",
    "        writer.write(dict(query=query, pos=pos, neg=neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c895f9-9ef4-4edc-b65d-722188eaa8bd",
   "metadata": {},
   "source": [
    "### Mine hard negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b73cf693-4138-429f-8188-0a72b36ed44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 3*GPUs----------\n"
     ]
    }
   ],
   "source": [
    "model = FlagModel(\n",
    "    model_name,\n",
    "    query_instruction_for_retrieval=query_prefix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adc677e6-c28f-49f9-a812-5cd4e93084b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferencing embedding for corpus (number=51135)--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [01:45<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferencing embedding for queries (number=125705)--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:46<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create index and search------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1965/1965 [00:04<00:00, 428.59it/s]\n"
     ]
    }
   ],
   "source": [
    "find_knn_neg(\n",
    "    model,\n",
    "    input_file=training_input_data,\n",
    "    candidate_pool=None,\n",
    "    output_file=training_hn_data,\n",
    "    sample_range=list(range(2, 200)),\n",
    "    negative_number=15,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
